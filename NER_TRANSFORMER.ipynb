{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf3ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras import optimizers\n",
    "import re\n",
    "import nltk\n",
    "import snowballstemmer\n",
    "stemmer = snowballstemmer.stemmer('tamil')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a283574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset naamapadam (C:/Users/dhanu/.cache/huggingface/datasets/ai4bharat___naamapadam/ta/1.0.0/c1b045180d60b208d2468bdad897d04461f08c7137c04a85220697b1bef7df9a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853a57edadf94f328bfedf6c6d8ead45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset,list_datasets\n",
    "a=load_dataset(\"ai4bharat/naamapadam\",\"ta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10235bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 497882\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 758\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 2795\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859b8d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "நல்கொண்டா\t5\n",
      "பகுதியிலேயே\t0\n",
      "இந்த\t0\n",
      "சம்பவம்\t0\n",
      "பதிவாகியுள்ளது\t0\n",
      ".\t0\n"
     ]
    }
   ],
   "source": [
    "idx=1\n",
    "rec=a['train'][5]\n",
    "for w, t in zip(rec['tokens'],rec['ner_tags']):\n",
    "  print('{}\\t{}'.format(w,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3191f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokens', 'ner_tags']\n",
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "column_names = a[\"train\"].column_names\n",
    "print(column_names)\n",
    "\n",
    "features = a[\"train\"].features\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa9d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iob_tag={0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f10674a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 497882\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf4e6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_list(data):\n",
    "    x,y=[],[]\n",
    "    for i in data:\n",
    "        x.append(i['tokens'])\n",
    "        t1=[]\n",
    "        for j in i[\"ner_tags\"]:\n",
    "            t1.append(iob_tag[j])\n",
    "        y.append(t1)\n",
    "    return x,y\n",
    "\n",
    "x_train,y_train=data_list(a[\"train\"])\n",
    "x_test,y_test=data_list(a[\"test\"])\n",
    "x_val,y_val=data_list(a[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5add3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data,y):\n",
    "    l=[]\n",
    "    l1=[]\n",
    "    for i,j in zip(data,y):\n",
    "        t=[]\n",
    "        t1=[]\n",
    "        for k,m in zip(i,j):\n",
    "            x=re.sub(\"\\W\",\"\",k)\n",
    "            if x==\"\":\n",
    "                continue\n",
    "            t.append(x)\n",
    "            t1.append(m)\n",
    "        l.append(t)\n",
    "        l1.append(t1)\n",
    "    return l,l1\n",
    "x_train,y_train=pre_process(x_train,y_train)\n",
    "x_test,y_test=pre_process(x_test,y_test)\n",
    "x_val,y_val=pre_process(x_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d75d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare english tokenizer\n",
    "def tokenize(data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    size_vocab= len(tokenizer.word_index) + 1\n",
    "    return tokenizer,size_vocab\n",
    "\n",
    "x_train_token,x_train_size=tokenize(x_train)\n",
    "y_train_token,y_train_size=tokenize(y_train)\n",
    "x_test_token,x_test_size=tokenize(x_test)\n",
    "y_test_token,y_test_size=tokenize(y_test)\n",
    "x_val_token,x_val_size=tokenize(x_val)\n",
    "y_val_token,y_val_size=tokenize(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bc80200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 91\n"
     ]
    }
   ],
   "source": [
    "def encode(tokenizer, m, data):\n",
    "    seq = tokenizer.texts_to_sequences(data)\n",
    "    seq = pad_sequences(seq, maxlen=m, padding='post')\n",
    "    return seq\n",
    "\n",
    "m=0\n",
    "data=x_train+x_val+x_test\n",
    "for i in data:\n",
    "    if m<len(i):\n",
    "        m=len(i)\n",
    "print(\"max length:\",m)\n",
    "\n",
    "X_train = encode(x_train_token, m, x_train)\n",
    "Y_train= encode(y_train_token, m, y_train)\n",
    "X_test = encode(x_test_token, m, x_test)\n",
    "Y_test= encode(y_test_token, m, y_test)\n",
    "X_val = encode(x_val_token, m, x_val)\n",
    "Y_val= encode(y_val_token, m, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a32167c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #encoder\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(x_train_size, units, input_length=in_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    #decoder\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(Dense(y_train_size, activation='softmax'))\n",
    "    model.summary()\n",
    "    rms = optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(optimizer=rms, loss='sparse_categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "    history = model.fit(X_train, Y_train, epochs=2, validation_data = (X_val,Y_val), verbose=1)\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f075ac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 91, 64)            22871552  \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " repeat_vector_2 (RepeatVect  (None, 91, 64)           0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 91, 64)            33024     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 91, 8)             520       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,938,120\n",
      "Trainable params: 22,938,120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "15559/15559 [==============================] - 1349s 86ms/step - loss: 0.0852 - accuracy: 0.9733 - val_loss: 0.1412 - val_accuracy: 0.9679\n",
      "Epoch 2/2\n",
      "15559/15559 [==============================] - 1354s 87ms/step - loss: 0.0576 - accuracy: 0.9806 - val_loss: 0.1526 - val_accuracy: 0.9651\n"
     ]
    }
   ],
   "source": [
    "model,history = model(x_train_size,y_train_size , m, m, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "990f5c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "model.save(\"nlp.h5\")\n",
    "densenet=load_model(\"nlp.h5\")\n",
    "p=densenet.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4b9b4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 22ms/step - loss: 0.1862 - accuracy: 0.9609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18620574474334717, 0.9608715772628784]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
